import pandas as pd
import xarray as xr
import numpy as np
from pathlib import Path
import os
from typing import List, Dict, Literal


class CaravanLoader:
    """
    A data loader for datasets processed in the Caravan format.

    This class is designed to read the time series data and attributes
    generated by the Caravan post-processing script. It provides a clean
    interface to load data for specific basin IDs, making it suitable for
    use within a PyTorch Lightning DataModule.

    Args:
        root_dir (str | Path): The root directory of the processed Caravan dataset.
            This is the `OUTPUT_DIR` from the processing notebook.
        basin_prefix (str): The short, descriptive string for the basin set
            (e.g., 'us', 'gb').
        timeseries_format (Literal['netcdf', 'csv']): The file format of the
            time series data. Defaults to 'netcdf'.
    """

    def __init__(
        self,
        basin_prefix: str,
        timeseries_format: Literal["netcdf", "csv"] = "netcdf",
    ):
        self.root_dir = Path(os.getenv("CAVARAN_DATA_PATH"))
        self.basin_prefix = basin_prefix
        self.timeseries_format = timeseries_format

        # Construct paths to data directories
        self.attributes_dir = self.root_dir / "attributes" / self.basin_prefix
        self.timeseries_dir = (
            self.root_dir / "timeseries" / self.timeseries_format / self.basin_prefix
        )

        # Validate paths
        if not self.root_dir.is_dir():
            raise FileNotFoundError(f"Root directory not found: {self.root_dir}")
        if not self.attributes_dir.is_dir():
            raise FileNotFoundError(
                f"Attributes directory not found: {self.attributes_dir}"
            )
        if not self.timeseries_dir.is_dir():
            raise FileNotFoundError(
                f"Time series directory not found: {self.timeseries_dir}"
            )

        # Load and merge all attributes once during initialization
        self._load_attributes()

        # Get the variable names from a sample file
        self._get_variable_names()

    def _load_attributes(self):
        """Loads and merges all attribute files into a single DataFrame."""
        try:
            df_other = pd.read_csv(
                self.attributes_dir / f"attributes_other_{self.basin_prefix}.csv",
                index_col="gauge_id",
            )
            df_caravan = pd.read_csv(
                self.attributes_dir / f"attributes_caravan_{self.basin_prefix}.csv",
                index_col="gauge_id",
            )
            df_hydroatlas = pd.read_csv(
                self.attributes_dir / f"attributes_hydroatlas_{self.basin_prefix}.csv",
                index_col="gauge_id",
            )

            # Merge all attributes into a single dataframe
            self.attributes = pd.concat([df_other, df_caravan, df_hydroatlas], axis=1)
            self.available_basins = self.attributes.index.tolist()
            print(
                f"✅ Successfully loaded attributes for {len(self.available_basins)} basins."
            )

        except FileNotFoundError as e:
            print(f"Error loading attribute files: {e}")
            raise

    def _get_variable_names(self):
        """Reads one time series file to get the column names/order."""
        if not self.available_basins:
            self.timeseries_variables = []
            return

        sample_basin_id = self.available_basins[0]
        file_extension = ".nc" if self.timeseries_format == "netcdf" else ".csv"
        sample_file_path = self.timeseries_dir / f"{sample_basin_id}{file_extension}"

        if not sample_file_path.exists():
            self.timeseries_variables = []
            print(
                f"⚠️ Warning: Could not find sample file to determine variable names: {sample_file_path}"
            )
            return

        if self.timeseries_format == "netcdf":
            df = xr.open_dataset(sample_file_path).to_dataframe()
        else:  # csv
            df = pd.read_csv(sample_file_path, index_col="date", parse_dates=True)

        self.timeseries_variables = df.columns.tolist()

    @property
    def basins(self) -> List[str]:
        """Returns a list of all available basin IDs."""
        return self.available_basins

    def get_attributes(self, basin_ids: List[str]) -> pd.DataFrame:
        """
        Retrieves static attributes for a given list of basin IDs.

        Args:
            basin_ids (List[str]): A list of basin IDs.

        Returns:
            pd.DataFrame: A DataFrame containing the merged attributes for
                          the requested basins, indexed by basin ID.
        """
        missing_basins = set(basin_ids) - set(self.available_basins)
        if missing_basins:
            raise ValueError(
                f"Attributes not found for the following basins: {list(missing_basins)}"
            )

        return self.attributes.loc[basin_ids]

    def load_basin_data(self, basin_ids: List[str]) -> Dict[str, np.ndarray]:
        """
        Loads time series data for one or more basins.

        Args:
            basin_ids (List[str]): A list containing one or more basin IDs to load.

        Returns:
            Dict[str, np.ndarray]: A dictionary where keys are the basin IDs and
                                   values are NumPy arrays of the time series data.
                                   Each array has shape (n_timesteps, n_variables).
        """
        data_dict = {}
        missing_basins = set(basin_ids) - set(self.available_basins)
        if missing_basins:
            raise ValueError(
                f"Time series data not found for the following basins: {list(missing_basins)}"
            )

        print(f"Loading time series data for {len(basin_ids)} basin(s)...")
        for basin_id in basin_ids:
            file_extension = ".nc" if self.timeseries_format == "netcdf" else ".csv"
            file_path = self.timeseries_dir / f"{basin_id}{file_extension}"

            if not file_path.exists():
                print(
                    f"⚠️ Warning: File not found for basin '{basin_id}' at {file_path}. Skipping."
                )
                continue

            try:
                if self.timeseries_format == "netcdf":
                    # xarray is great for netCDF
                    ds = xr.open_dataset(file_path)
                    df = ds.to_dataframe()
                else:  # 'csv'
                    df = pd.read_csv(file_path, index_col="date", parse_dates=True)

                # Ensure consistent column order
                df = df[self.timeseries_variables]

                # Convert to numpy array and store
                data_dict[basin_id] = df.to_numpy()
            except Exception as e:
                print(
                    f"❌ Error loading or processing file for basin '{basin_id}': {e}"
                )

        return data_dict
